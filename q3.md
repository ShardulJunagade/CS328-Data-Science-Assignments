# K-means and K-median Clustering on 1D Gaussian Mixture Data

## Dataset Creation

In this notebook, we create a 1-dimensional dataset by sampling from two Gaussian distributions:
- 100 samples from a Gaussian with $\mu = 0, \sigma^2 = 1$
- 100 samples from a Gaussian with $\mu = 3, \sigma^2 = 1$

This creates a dataset with two distinct clusters that we'll attempt to recover using clustering algorithms.

## K-means Clustering Analysis ($k = 2$)

K-means clustering is an algorithm that partitions data into $k$ clusters by minimizing the sum of squared distances between data points and their assigned cluster centers. The objective function being minimized is:

$$
\min_{c_1, c_2, \ldots, c_k} \sum_{i=1}^{n} \min_{j \in \{1, 2, \ldots, k\}} ||x_i - c_j||^2
$$

For our case with $k = 2$, we're minimizing:

$$
\min_{c_1, c_2} \sum_{i=1}^{n} \min_{j \in \{1, 2\}} ||x_i - c_j||^2
$$

Where:
- $x_i$ is the $i$-th data point
- $c_j$ is the center of the $j$-th cluster
- $||x_i - c_j||^2$ is the squared Euclidean distance

We use scikit-learn's implementation of k-means, which:
1. Initializes cluster centers (using k-means++ by default)
2. Assigns each point to the nearest center
3. Recalculates centers as the mean of all points in each cluster
4. Repeats steps 2-3 until convergence

## K-median Clustering (Exhaustive Search)

Unlike k-means, k-median clustering minimizes the sum of absolute distances (Manhattan/L1 distance) rather than squared distances:

$$
\min_{c_1, c_2, \ldots, c_k} \sum_{i=1}^{n} \min_{j \in \{1, 2, \ldots, k\}} |x_i - c_j|
$$

For our 1D data with $k = 2$, we perform an exhaustive search by:
1. Sorting the 200 data points
2. Testing every possible split point $s$ (putting $s$ points in the first cluster and $200-s$ in the second)
3. Computing cluster centers as the median of points in each cluster
4. Calculating the objective function value for each split
5. Selecting the split that minimizes the total distance

This approach guarantees finding the global optimum for the k-median objective in 1D data.

## Comparison of Methods

The key differences between k-means and k-median are:

1. **Objective Function**:
   - K-means minimizes squared distances (L2 norm): $\sum_{i=1}^{n} \min_{j} ||x_i - c_j||^2$
   - K-median minimizes absolute distances (L1 norm): $\sum_{i=1}^{n} \min_{j} |x_i - c_j|$

2. **Center Calculation**:
   - K-means centers are computed as the mean of points in each cluster
   - K-median centers are computed as the median of points in each cluster

3. **Robustness to Outliers**:
   - K-means is more sensitive to outliers due to the squaring operation
   - K-median is more robust to outliers as it uses absolute distances

By comparing the accuracy (fraction of correctly classified points) and the cluster centers found by each method, we can determine which approach works better for this particular dataset.