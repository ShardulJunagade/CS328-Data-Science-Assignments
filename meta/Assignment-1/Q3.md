## Question 3

Q3. Create a random variable $X$ for which Markovâ€™s inequality is tight. Give proof for your answer. If it is tight, then why are we using other inequalities e.g. Chebyshev and Chernoff?

### Solution

**Markov's Inequality** states that for any non-negative random variable $X$ and any $c > 0$, we have:
$$
P(X \geq c) \leq \frac{E[X]}{c}
$$

Let $X$ be a random variable that takes values $0$ and $a>0$. The probability mass function of $X$ is given by:
$$
P(X = x) = 
\begin{cases} 
1 - p & \text{if } x = 0 \\
p & \text{if } x = a
\end{cases}
$$

The expectation of $X$ is given by:
$$
E[X] = 0 \times (1 - p) + a \times p = ap
$$

Then by Markov's Inequality:
$$
P(X \geq c) \leq \frac{E[X]}{c} = \frac{ap}{c}
$$

If we choose $c = a$, then:
$$
P(X \geq a) \leq \frac{ap}{a} = p
$$

This is tight because the equality holds. The probability that $X$ is greater than or equal to $a$ is exactly $p$. This is the maximum probability that Markov's Inequality can give.


**Use of Other Inequalities (Chebyshev and Chernoff)**

- Markov's Inequality is tight only for specific distributions (e.g. the one we constructed above). For other distributions, Markov's Inequality may not be tight. It only uses the first moment of the random variable and does not take into account higher moments or the distribution shape.

- Chebyshev's Inequality is more general and provides a tighter bound than Markov's Inequality for any random variable with finite variance as it uses the second moment (variance) of the random variable. It is useful when we want a more precise estimate of the tail probabilities.
$$
P(|X - E[X]| \geq c) \leq \frac{Var[X]}{c^2}
$$


- Chernoff's Inequality is even tighter than Chebyshev's Inequality because it uses the moment generating function of the random variable. It provides exponentially decreasing bounds on tail probabilities. The inequalities are given by:

    - Upper Tail:
    $
    P(X \geq (1 + \delta)\mu) \leq e^{-\frac{\delta^2 \mu}{2 + \delta}}
    $
    for all $\delta > 0$;

    - Lower Tail:
    $
    P(X \leq (1 - \delta)\mu) \leq e^{-\frac{\delta^2 \mu}{2}}
    $
    for all $0 < \delta < 1$
